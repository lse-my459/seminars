{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 1: Solutions to Exercises\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan HÃ¼bert**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Directory Management\n",
    "\n",
    "1. In the following code chunk, create an object called `edir` that has the fill path to the directory where you will store your work on these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "edir = os.path.join(os.path.expanduser(\"~\"), \"LSE-MY459-WT26\", \"SeminarWeek02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the following code chunk, download the file `news_article.txt` from the data repo on the course GitHub, and save it into `edir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "rfile = \"https://raw.githubusercontent.com/lse-my459/data/master/news_article.txt\"\n",
    "\n",
    "# Where will we store the local copy of it?\n",
    "lfile = os.path.join(edir, os.path.basename(rfile))\n",
    "\n",
    "# Check if you have the file yet and if not, download it to correct location\n",
    "if not os.path.exists(lfile):\n",
    "    r = requests.get(rfile) # make GET request for the remote file\n",
    "    r.raise_for_status()    # raise exception if there's an HTTP error\n",
    "    \n",
    "    # Write the raw bytes received from the server to the local file path\n",
    "    with open(lfile, \"wb\") as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fun with Digital Text\n",
    "\n",
    "1. Read `news_article.txt` into Python as raw bytes, and then echo the first 10 bytes of the resulting `bytes` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\"\\xe6\\xc1\\xce\\xd4\\xc1\\xda\\xc9\\xc9 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(lfile, 'rb') as f:\n",
    "    raw = f.read()\n",
    "raw[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to decode the `bytes` object using the UTF-8 and UTF-16 encoding standards and the `bytes.decode()` method. Is the file you read into Python UTF-8 or UTF-16 encoded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is not UTF-8 encoded, see following error: 'utf-8' codec can't decode byte 0xe6 in position 1: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "# Note: this try-except will try to decode, and if there is an error, it prints the error. You can also just have `raw.decode(\"utf-8\")` in this chunk and see it returns \n",
    "# an error, although it would create some complications for running all cells in this file at once (due to the error)\n",
    "try:\n",
    "    raw.decode(\"utf-8\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"This data is not UTF-8 encoded, see following error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data is not UTF-16 encoded, see following error: 'utf-16-le' codec can't decode bytes in position 26-27: illegal UTF-16 surrogate\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raw.decode(\"utf-16\")\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"This data is not UTF-16 encoded, see following error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, use the `charset_normalizer` module to try to discover the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'KOI8-R', 'language': 'Bulgarian', 'confidence': 0.995}\n"
     ]
    }
   ],
   "source": [
    "import charset_normalizer\n",
    "print(charset_normalizer.detect(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now try to decode the `bytes` object using the best guess you get from the `charset_normalizer` module. Print the decoded text using the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ð¤Ð°Ð½Ñ‚Ð°Ð·Ð¸Ð¸ ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð°Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ\": 105 Ð»ÐµÑ‚ ÑÐ¾ Ð´Ð½Ñ Ñ€Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¤ÐµÐ´ÐµÑ€Ð¸ÐºÐ¾ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸\n",
      "105 Ð»ÐµÑ‚ Ð½Ð°Ð·Ð°Ð´ Ñ€Ð¾Ð´Ð¸Ð»ÑÑ Ð¤ÐµÐ´ÐµÑ€Ð¸ÐºÐ¾ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸\n",
      "20 ÑÐ½Ð²Ð°Ñ€Ñ 2025, 09:30\n",
      "20 ÑÐ½Ð²Ð°Ñ€Ñ Ð¸ÑÐ¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ 105 Ð»ÐµÑ‚ ÑÐ¾ Ð´Ð½Ñ Ñ€Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ ÐºÑƒÐ»ÑŒÑ‚Ð¾Ð²Ð¾Ð³Ð¾ Ð¸Ñ‚Ð°Ð»ÑŒÑÐ½ÑÐºÐ¾Ð³Ð¾ Ñ€ÐµÐ¶Ð¸ÑÑÑ‘Ñ€Ð° Ð¤ÐµÐ´ÐµÑ€Ð¸ÐºÐ¾ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸. Ð‘Ð¾Ð»ÑŒÑˆÐ¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð½Ð° ÐµÐ³Ð¾ ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ Ñ†Ð¸Ñ€ÐºÐ¾Ð²Ð¾Ðµ Ð¸ÑÐºÑƒÑÑÑ‚Ð²Ð¾ Ð¸ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°: Ð´Ð¾Ð»Ð³Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸ Ñ‚Ñ€ÑƒÐ´Ð¸Ð»ÑÑ ÐºÐ°Ñ€Ð¸ÐºÐ°Ñ‚ÑƒÑ€Ð¸ÑÑ‚Ð¾Ð¼. Ð•Ð³Ð¾ ÐºÐ°Ñ€ÑŒÐµÑ€Ð° Ð² ÐºÐ¸Ð½Ð¾ Ð½Ð°Ñ‡Ð°Ð»Ð°ÑÑŒ Ñ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ², Ð² Ñ‚Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ Ðº Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð¸Ð· Ñ„Ð¸Ð»ÑŒÐ¼Ð¾Ð² Ð Ð¾Ð±ÐµÑ€Ñ‚Ð¾ Ð Ð¾ÑÑÐµÐ»Ð»Ð¸Ð½Ð¸. ÐŸÐµÑ€Ð²Ñ‹Ðµ Ñ€ÐµÐ¶Ð¸ÑÑÑ‘Ñ€ÑÐºÐ¸Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸ Ð±Ñ‹Ð»Ð¸ Ð²ÑÑ‚Ñ€ÐµÑ‡ÐµÐ½Ñ‹ Ð¾Ð²Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð½Ð° Ñ„ÐµÑÑ‚Ð¸Ð²Ð°Ð»ÑÑ…, Ð° Ð»ÐµÐ½Ñ‚Ñ‹ \"ÐÐ¾Ñ‡Ð¸ ÐšÐ°Ð±Ð¸Ñ€Ð¸Ð¸\", \"Ð ÐµÐ¿ÐµÑ‚Ð¸Ñ†Ð¸Ñ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°\", \"8 1/2\" Ð¸ \"ÐÐ¼Ð°Ñ€ÐºÐ¾Ñ€Ð´\" ÑÑ‚Ð°Ð»Ð¸ ÐºÐ»Ð°ÑÑÐ¸ÐºÐ¾Ð¹ Ð¼Ð¸Ñ€Ð¾Ð²Ð¾Ð³Ð¾ ÐºÐ¸Ð½Ð¾ Ð¸ Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð¼ Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¾ÐºÐ¾Ð»ÐµÐ½Ð¸Ð¹ ÐºÐ¸Ð½ÐµÐ¼Ð°Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸ÑÑ‚Ð¾Ð². Ð¤ÐµÐ´ÐµÑ€Ð¸ÐºÐ¾ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸ Ð¾Ð±Ð»Ð°Ð´Ð°Ñ‚ÐµÐ»ÑŒ Ð¿ÑÑ‚Ð¸ Ð¿Ñ€ÐµÐ¼Ð¸Ð¹ \"ÐžÑÐºÐ°Ñ€\" Ð¸ Ð»Ð°ÑƒÑ€ÐµÐ°Ñ‚ ÐšÐ°Ð½Ð½ÑÐºÐ¾Ð³Ð¾ Ð¸ Ð’ÐµÐ½ÐµÑ†Ð¸Ð°Ð½ÑÐºÐ¾Ð³Ð¾ ÐºÐ¸Ð½Ð¾Ñ„ÐµÑÑ‚Ð¸Ð²Ð°Ð»ÐµÐ¹. ÐŸÐ¾ ÑÐµÐ¹ Ð´ÐµÐ½ÑŒ Ð¾Ð½ ÑÑ‡Ð¸Ñ‚Ð°ÐµÑ‚ÑÑ Ð¾Ð´Ð½Ð¸Ð¼ Ð¸Ð· Ð²ÐµÐ»Ð¸Ñ‡Ð°Ð¹ÑˆÐ¸Ñ… Ñ€ÐµÐ¶Ð¸ÑÑÑ‘Ñ€Ð¾Ð² Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸.\n"
     ]
    }
   ],
   "source": [
    "print(raw.decode(encoding='KOI8-R'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save a new version of the file called `news_article_utf8.txt`, where the text you've just decoded is re-encoded as UTF-8 when saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lfile.replace(\".txt\", \"utf8.txt\"), 'w', encoding='utf-8') as f:\n",
    "    f.write(raw.decode(encoding='KOI8-R'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Try to read the newly created file in to Python to verify that it is indeed encoded as UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ð¤Ð°Ð½Ñ‚Ð°Ð·Ð¸Ð¸ ÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð°Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ\": 105 Ð»ÐµÑ‚ ÑÐ¾ Ð´Ð½Ñ Ñ€Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¤ÐµÐ´ÐµÑ€Ð¸ÐºÐ¾ Ð¤ÐµÐ»Ð»Ð¸Ð½Ð¸\n",
      "105 Ð»ÐµÑ‚ Ð½Ð°Ð·Ð°Ð´ Ñ€Ð¾Ð´Ð¸Ð»ÑÑ Ð¤\n"
     ]
    }
   ],
   "source": [
    "## If no error, then it is correct!\n",
    "utf8_file = lfile.replace(\".txt\", \"utf8.txt\")\n",
    "with open(utf8_file, 'r', encoding=\"utf-8\") as f:\n",
    "    print(f.read()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How many characters is in this file? How many bytes does it take to store it as a UTF-8 encoded file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853\n",
      "1539\n",
      "1539\n"
     ]
    }
   ],
   "source": [
    "## Count characters by counting length of string after decoding\n",
    "print(len(open(utf8_file, 'r').read())) # read and decode bytes and calculate len()\n",
    "\n",
    "## Get size in two ways\n",
    "## Option 1:\n",
    "print(os.path.getsize(utf8_file))\n",
    "## Option 2:\n",
    "print(len(open(utf8_file, 'rb').read())) # read raw bytes and calculate len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Trump Tweet Data\n",
    "\n",
    "1. Load the sparse matrix, vocabular and the tweet corpus you made in a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "dfm_sparse = sparse.load_npz(os.path.join(edir, 'tweet-dfm-sparse.npz'))\n",
    "vocabulary = open(os.path.join(edir, 'tweet-dfm-features.txt'), mode = \"r\").read().split(\"\\n\")\n",
    "corpus = pd.read_csv(os.path.join(edir, \"tweet-corpus.csv\"))\n",
    "corpus[\"datetime\"].head()\n",
    "corpus[\"datetime\"] = pd.to_datetime(corpus[\"datetime\"]).dt.tz_convert(\"US/Eastern\") # Convert to US Eastern time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove any tweet posted before Trump became president. Hint: all US presidents are inaugurated at 12:00 Eastern US time on the 20th of January of their first year in office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3722, 2)\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus.loc[corpus[\"datetime\"] >= pd.to_datetime(\"2017-01-20 12:00-05:00\"),:]\n",
    "print(corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = corpus.index.to_numpy()\n",
    "dfm_sparse = dfm_sparse[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Trim the dataset features by dropping any token that does not occur in at least 5 documents, and at least 8 times total in the corpus. How many features did you trim out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4770 features.\n",
      "New DFM has shape: (3722, 1179)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "docf = (dfm_sparse > 0).sum(axis=0).A1\n",
    "ttf = dfm_sparse.sum(axis=0).A1\n",
    "\n",
    "min_docf = 5\n",
    "min_ttf = 8\n",
    "\n",
    "cols_keep = np.where((docf >= min_docf) & (ttf >= min_ttf))[0]\n",
    "print(f\"Removed {dfm_sparse.shape[1] - len(cols_keep)} features.\")\n",
    "\n",
    "vocabulary_trimmed = [vocabulary[x] for x in cols_keep]\n",
    "dfm_trimmed = dfm_sparse[:, cols_keep]\n",
    "print(f\"New DFM has shape: {dfm_trimmed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Basic Text Manipulations\n",
    "\n",
    "1. Display Trump's first tweet as president. Do this in three chunks: one where you echo the tweet, one where you print the tweet, and one where you pretty print the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today we are not merely transferring power from one Administration to another, or from one party to another â€“ but we are transferring...'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we are not merely transferring power from one Administration to another, or from one party to another â€“ but we are transferring...\n"
     ]
    }
   ],
   "source": [
    "print(corpus[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Today we are not merely transferring power from one Administration to '\n",
      " 'another, or from one party to another â€“ but we are transferring...')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(corpus[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the tweet posted at 12:55 pm Eastern US time on the day that Trump was first inaugurated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will follow two simple rules: BUY AMERICAN &amp; HIRE AMERICAN!\n",
      "#InaugurationDay #MAGAðŸ‡ºðŸ‡¸\n"
     ]
    }
   ],
   "source": [
    "# Find tweet at 17:55 UK time on Jan 20, 2017\n",
    "tweet = corpus.loc[corpus[\"datetime\"] >= pd.to_datetime(\"2017-01-20 12:55-05:00\"),:]\n",
    "tweet = tweet[\"text\"].iloc[0]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenise the tweet using whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'will', 'follow', 'two', 'simple', 'rules:', 'BUY', 'AMERICAN', '&amp;', 'HIRE', 'AMERICAN!', '#InaugurationDay', '#MAGAðŸ‡ºðŸ‡¸']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tok = re.split(r'\\s+', tweet)\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Replace any emojis with a descriptive placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'will', 'follow', 'two', 'simple', 'rules:', 'BUY', 'AMERICAN', '&amp;', 'HIRE', 'AMERICAN!', '#InaugurationDay', '#MAGA<us_flag_emoji>']\n"
     ]
    }
   ],
   "source": [
    "# Replace emoji patterns (MAGA flag emoji example)\n",
    "tok = [re.sub(r'[#]MAGA.+', '#MAGA<us_flag_emoji>', t) for t in tok]\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pre-process the tweet by: standardising the cases as appropriate, cleaning out non-words, punctuation and numbers, and any other step you need to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'follow', 'two', 'simple', 'rules', 'buy', 'american', 'hire', 'american', '#inaugurationday', '#maga<usflagemoji>']\n"
     ]
    }
   ],
   "source": [
    "# Make lowercase\n",
    "tok = [x.lower() for x in tok]\n",
    "    \n",
    "# Remove HTML entities like &amp; &lt; etc.\n",
    "tok = [x for x in tok if not re.match(r'^&#?[a-z]+;$',x)]\n",
    "\n",
    "# Remove punctuation but keep # < >\n",
    "tok = [re.sub(r'[^a-z#<> ]', '', x) for x in tok]\n",
    "    \n",
    "# Remove excess whitespace\n",
    "tok = [x.strip() for x in tok]\n",
    "    \n",
    "# Remove empty strings\n",
    "tok = [t for t in tok if t]  \n",
    "\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load English stop words from the `nltk` module and remove any stop word from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follow', 'two', 'simple', 'rules', 'buy', 'american', 'hire', 'american', '#inaugurationday', '#maga<usflagemoji>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "tok = [x for x in tok if x not in sw]\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Stem words using the Snowball stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['follow', 'two', 'simpl', 'rule', 'buy', 'american', 'hire', 'american', '#inaugurationday', '#maga<usflagemoji>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "sstemmer = snowball.SnowballStemmer(\"english\")\n",
    "tok = [sstemmer.stem(x) for x in tok]\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Using a `Counter` object, calculate the term frequencies of this tweet. Echo the `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'american': 2,\n",
       "         'follow': 1,\n",
       "         'two': 1,\n",
       "         'simpl': 1,\n",
       "         'rule': 1,\n",
       "         'buy': 1,\n",
       "         'hire': 1,\n",
       "         '#inaugurationday': 1,\n",
       "         '#maga<usflagemoji>': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tok_count = Counter(tok)\n",
    "tok_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Turn this `Counter` object into a row of a Pandas `DataFrame` object and into a row of a SciPy sparse array object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>follow</th>\n",
       "      <th>two</th>\n",
       "      <th>simpl</th>\n",
       "      <th>rule</th>\n",
       "      <th>buy</th>\n",
       "      <th>american</th>\n",
       "      <th>hire</th>\n",
       "      <th>#inaugurationday</th>\n",
       "      <th>#maga&lt;usflagemoji&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   follow  two  simpl  rule  buy  american  hire  #inaugurationday  \\\n",
       "0       1    1      1     1    1         2     1                 1   \n",
       "\n",
       "   #maga<usflagemoji>  \n",
       "0                   1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tok_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 9 stored elements and shape (1, 9)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t2.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 8)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['#inaugurationday', '#maga<usflagemoji>', 'american', 'buy',\n",
       "       'follow', 'hire', 'rule', 'simpl', 'two'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer()\n",
    "print(dv.fit_transform(tok_count))\n",
    "\n",
    "vocabulary = dv.get_feature_names_out() # preserve the vocab!\n",
    "vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
